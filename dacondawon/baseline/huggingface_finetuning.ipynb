{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output \n",
    "import time\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "        print(\"full dataset size : \",len(self.data))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = self.data.iloc[idx, 1]    # default : ./train_img/TRAIN_0000.png\n",
    "        img_path = \"../data\"+self.data.iloc[idx, 1][1:]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        assert False , \"SatelliteDataset class must be used as test dataset obj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TV_SatelliteDataset(Dataset):\n",
    "    def __init__(self,transform=None, is_train = True):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return 114240-11424  ###### 조심할것.\n",
    "        return 11424\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_train:\n",
    "            idx += 102816\n",
    "        img_path = \"../split_data_224/train_img\"+f\"/{idx//16}_{idx%16}.png\"\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask_path = \"../split_data_224/train_mask\"+f\"/{idx//16}_{idx%16}.png\"\n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = TV_SatelliteDataset(transform=transform, is_train=True)\n",
    "val_dataset = TV_SatelliteDataset(transform=transform, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\", id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## huggingface cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/leadawon5/.cache/huggingface/datasets/segments___parquet/segments--sidewalk-semantic-2-007b1ee78ca1e890/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff89e49d58334300bdba20bbfa548c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset_identifier = \"segments/sidewalk-semantic\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(hf_dataset_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/leadawon5/.cache/huggingface/datasets/segments___parquet/segments--sidewalk-semantic-2-007b1ee78ca1e890/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-cddc58c3a7b554a7.arrow\n"
     ]
    }
   ],
   "source": [
    "ds = ds.shuffle(seed=1)\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1080>,\n",
       " 'label': <PIL.PngImagePlugin.PngImageFile image mode=L size=1920x1080>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB4AAAAQ4CAAAAADNuJ6fAAAmn0lEQVR4nO3dXW/cWHot4O243XZ00dJVAswcTPv//yq3gTM4cS5SMhDFnj4e50KfVWJVkSySa2/yeYCZtqRSabtseWm93CTf/J/S4a7rnY8+nPrggX8Z8NiuBbzff/f3g4ddjXz6Bpz8IzjmQymlfJt2IQ/u/yQO/wAmet7pn7YaVfz+Hr+N0utYjTdnH/FzgVVc5rf0AijjExIAuMAv6QVc5G7FFfi1F6OHrpb74cV/52nBAEynO4CvRg1AAx7XufYgPhz7f3gVsR9evSWEIaP+ATR1MIIGgIBKR9BX3ZuQuovu3X0TXnEL7tj39kHFBUb7ahdWBSoN4Ic4HTQJ39YB4dI1h97/sIAGjvlqJ3SeETQABBxpwEdGwEu7+jHk0XcrnUIfP/F6wZL7/fCcbKBxX4sWnKUBA0CAAGaA92owwETWFcBVzM2r8WHIRUMBWNa6AhgAGnE0gK+a3NB0pwO3yGAb2KATDXiKBP7nPyd4kkHWEcHr+F0AcJwRNAAEVHslrPHu1nZVym/HzwR+vhjWiQcBUCENGAACTgVwm/uwSnE6EgDVq3wE/bbsXYzyx9uuB3VcN3Pld0cCoHlG0AAQcDqAO0tkK3e5a3gOPWrprfy5AFDK2RF0512RbLit0+Gfy/FE3vvBKviDyvXjIq9fvPM2s5bGXe+9dfviPV5PqJURNAAEjNuE9dit7hvXz/s33kywnJGuOlvcWu8PfMx1+f7yzY4C3Pl6HL7zWCP+PvElI+8b2uthyrXSNtj1mbeZVPCfOtZFAwaAgLMNuPMo8KP7kvXQi15m+eKXgO52t60KXN4/V+D7P5oxv//9z9n7099r2JfR0TjwWylf02uARc10HnBnsf7n0Y9c7OSPCe37dmzX2+GY+f3Lj1yVHz9KKZ3nTvd2ldilZQpdrd96Pm5olP728P+riOCf6QXQCiNoAAjo0YC7dziNdT+cnj73V96BR5hs/B55aa914P6Wmuf3rb+PD+1dZwc8MaxI6FKU/1yoeje5Ebr/qdcnrr0x6e97sjm0E8gbNjwl7z/jXAxLXzbLCBoAAiq/GQOP0pcfm2QOPei3cG0G3c+ZAfQ0r+MFNfXxU480YQWY7dKAASBgRQ342OWwWjwM/Nr167Nwl7v7wruHFeybu6Hah1WNSVqqqgsHVhTAq/f+IIIfEvEppW4fH3bpub8H3h15//XeVydikQ3QovMlF6JkMkbQABDQowHPcxLoDCcirf9U4ElvhtDXsQJ8776B6cEVu77wYIX+CzM5G8DnM+161Ld33/h9W368eOvH6eFq52HgVRwCrtzhNJxlLDGAlr8wFyNoAAg404B79N+nX/04t/PnXxI3SdJ/L3N6AH3APHpZCxRg/RfmowEDQMCpBtxjS9Pej+AVnHJ7uA8rvZ6lXZWyd8w8otfpSd/P7yjTpOP0X5jTFOcBf3jeZXlXSp/Um7F3by1yn9X1Oz+fwt9PJrD0rYD87fLTmcBMxQgaAALmuBLWwxS4o5GJ+z5GnM9cV/t98niEorvOPkyhv5XXd2nQf3u5nXUXlv4LM7ssgE9+/z+lSKXp0Jbbpxf7cd7/4dtDcM34+g7aA33c49IPr2bdHSDSd6xJA1n+wtx0UgAIONWAp7q0YwW7o9fgsRg+7l26fX//voZe2/ulv+jBtw/vVHrHuT0YQh2+PZ7+C/PTgAEg4PQx4P1ydUEfvmuppzWkyVf1qcJHV7EStwc7MQ7fHkf/hSUM2YR1OJI+/EY/eQN1ETy1qyNX3HhbwbU4enn4+3PZrXromEOXUvauCzr0LlryFxZhBA0AAdOeB3x9aqp4pwL31XlbxfIwce5Rbx9vi/G2hSr8obw+P4lBOk8/MuEP+ZleAO24IIC7vulPTqG5SODnl4lOAz5v78KVva4lzUtrebW+phfQhwtRMhkjaAAImONSlMfYh9XbJu/q9DA+uX54YzW1DqCTBgwAAZM34NPnIo1pck3sJJrXJgpwKeV674wax4JTvjoPCZYwOoDHne1/+RT6x9MW38049pKt8IV4/bdKCgNrZQQNAAFDGnDPS1GeOhl4K7PU6XjFih1ZwCoNCODel4I+fhhYmgx0/AVb4fz5JKPoJX11NUpYgBE0AAT0bsCn78PQh/o7WPYlW+w6WL29uL8Ag7neJ1RGAwaAgHmuhPVqH5b2O62tHQEGWJ8FLkUpfEfywnVzxw9gFYygASCgdwO+erkP6+werMeSosTNY4PXA2MqD9+Up08rdCISzG/ICPrwFj0nXZdb6XsRLx9zuHrx3wHf0MDkjKABIGDYJqyru1LG3oeBQfTfE05d7bQe16WU8i29ipO0YEjSgAEgYJoA/lBKKeXb/k/7ivJ83tqDVb3DOxvXyqwFUgaeBzxoHxazeGsLdAOuD96qZWheddx+TS8AlmUEDQABg6+EdVXubhceqb0tPxb9erXTf6v3/A3y4ek9NXTgzv5rrDWpn+kFDOJs76xxDfjYvyW3twv8KyONaVHVB4KBACNoAAgYcTOGq7vXFfj9068eP1L1Zg+YU2tl96prBv3VdBJmpgEDQMCYAL7SbmGw63gxPvqN2/kt/dVZQTCvBe4HTNYPu6aXFc9ZoA1G0AAQoAHDjL6V8nwycMWcDAzLGxfAr44YvT43984+aGZTx2Utzru9Pz+gjsXW/g3pkDObYwQNAAHTjqCffsbWf9msx0u11tF7gWppwAAQMNMmLP2XDbud80yk7uc+rNt734Eunz6hN+kFsCJ2QcPk5pg+nw71x4/ejvzp1x5oWJ4RNAAEaMDQgL5D7etS/px1IcBkJgrgtw4zwWyGHFOeMH/dEAlmZQQNAAGTBfDbOS/573YCbJnbO8AqacAAEDDhJqzFauoPjRimVMNJSC4FzfZowAAQIIABIMB5wLAaTgGGlgjgVbvo/Ow/y7vpVrJV3+/vCLyM0/nrFmUL+JleAE0xggaAAAEMtXNnYVglAQwAAY4Bw6y+vy/vPzy9pcwCjwQwrIMt0AzkZhtpRtAAEKABr9wmbxT54syf71M+77fHX3w49ahzTzH+k4FVEcCrt+0LZ7+fIoK/nXrHsTx9eszhicDfqongp/O8r+PHpl0Jmi0yggaAAA2Y9fn+snS+n3gOPYFvc1Tgs3uw3FUYKqMBA0CABsz6vS/lghr86gjwwYc76+yZTzrqvrsPX6tzkKA9jTTgt9veScQEFrwpwlnfBubzbXqPFDCDRgIYANalzRH0D4WYwebajdVxWtHYAfSsbm3Dgrq0GcAwyvsyPIR7hOnUu5on+Tnhxc2cN3gtFmiBETQABGjAbMz7+c8KDkyg3x1shH537IFANTRgAAjQgOFi81/e+fb8layeS++frfVfV4JmmwQwLO51XO+l66VD8tbyFzbKCBoAAjRgCHJqblvepBfAqmjAUBup3KifP9MroC0CGAACBDBNutYSgcYJYAAIsAkLTpjrolYnC/yLmw8q+rBezQTw2/KjlOhNkO6SX3wBV+kFLOW/hjz4X+daBbB5RtAAENBMAw7X31LK1fpLMKPt1eqztfm/Lh4t3x683fSo2pUo2aiGArgCbWfw4b/Rh/+G16ZjfSdjZtBoua//ufyTHhf96cKlvFD7Hx3QhxE0AARowANdldJ0DX7hRZ1stFHN0nnn8GnYw3fPv+zu/LuO9/x+/nlvmx5Uw+powAAQoAGP0fax4Fo81+5RxayZ+vv/lvkyf5RyugXvys0yK6ERv6UXgAAeSQa/0iNQh4+6/3vvLSPUi+y8flATI2gACNCAx7taVwe+63sprD87PnfSlTz47/MPqcnHHo/51OMxf1y2jFEa3YQHjRPAlzCHnsDu3GT5H8us4xIfhzzYPd2BUooRNABEaMAXWtkcet+f7+Z41t3zL9/d/+/vf+l6XAPNt5SB7fdM/b10/PzHmbOBz30cWJIGDAABGvDFrlZcgae2K+W+9q7Gx4GPPtlxOz64TGfdlVK6zhL+usSpom7FwGYJ4Mu1MoW+zZ9G25m9jYya29AZ8P9+5uP3dp0ZDMzGCBoAAjTgKWx+Cr3bf7NX01Z8DyROAD6wK0UNhsUI4Em0MoVewqlDvEK3ATsZPNLP9AIG+upi0GlG0AAQoAEzgVVtbA6pYAINLEoDBoAADXgijgK/0H1lq3Z9nP4pL+q7yjKsggCeTPNboe9Kv/sh/efxD9lkNT1pC2tlBA0AARrwdLZ6c0K1lwu4EmWMs5DiBPCkrkrTIXy3O3jHqesQry53P6YXQO3cyplpGUEDQIAGPLk5N2PtLvv0Y5eIHLbPZ3XVFyBBAwaAAA14eg8n81R4LLhf03VZK4AFCODZPJ5UW2EQ7zsWuEbNADMyggaAAA14dvdNeJIevJviSZKN92P5NP8XefU1adrz9dmqHybBMAJ4EX8Pfu0Th3QNmanei+uj7p9gIJhpnhE0AARowOMla+0pxyuvxksvu6dfzX21wtMXojx2d5CD9zd/HxS2SgMGgAANuJRSb5ntZaMHeT9+Sq+A+XR334e7bve7bSZUTwC3Z6OBy3YcT1jZy5oYQQNAgAbcgqOdV+NldZRcNkMA18iQuZQ+V+34uMAqJjPsllPbJX/ZDiNoAAjQgGug8Xb7eMFHefYf6QVsws/0AmiPBgwAARpwKaWUvyx7JvDJO+5uuvOulOO/wGsCeAkCFzqcvhBlb65ESaOMoAEgQAOeg8bLSybQQAcBPIWTgStxN078Ap2MoAEgQAMeQ+OlNwV4ZrZg0SwNGAACNOBeTlfe9XbeTy43NdyqKu/n8rf0EmC1BHCXM3m73sCFtLuBd2MwgaZdRtAAEKABP3DqbpJRN49+K+X/H77PP1Osk7/ZnSQu3Pu8/EFg/yqxEUbQABDgZ80HOi90mbEAT3Avholu5wARGjAABAjgB3/9a3oFQNXepBfA2hhBA0e5DAfMRwMGgAANmKE+lk/nHuK83g36fP+fJTuzLVi0TQADR5wJ07382824DlgnI2gACBDAMKkV3Qxp0DT5ZqZFtOJnegE0SAADQIBjwE/++n/TK6B1K2q/Eb+mFwCLEsAwFfk72nP0fvsQXAYsyggaAAI0YLiY6vtowL4tJ/GyeQIYGGf3+IubEZ/8Mn8d+2WbjKABIEADfvbX0vo+6I9TP+GnI1/nyPvZqN2Yz7k58oFv9/+xF4v104ABIEADbqz2fqziq718/6fZV8Eq7U4fOv5WtGDWbpsB3FbmPvuYXkCXj8+//JRaA8v7fOS9A/ZB785t3xLCrJsRNAAEbKYBt1p6m2JzFgOdmUOXUr4pwazWugNY6sIsugfQY+xuzp4G/K3zvf+YbAkQYgQNAAGra8BKL0zk86AbAo/9pF35txFf5uHyWWowLdOAASBgHQ1Y7SVnxXdiGHRS0fMn9fisvWPIX8rIEqwG07ZmA1jmUoMVp28pZWQEn/2s11u4voyO4CKFaZYRNAAEtNWA1V5YWNdE+fxJSMOb85cLKnAp9zVYC6Yt1QewzH3h08f0Ci72sdnrVa592HzC51L6HNd99Undn3IsvS+aQt8zi6YtRtAAEFBjA1Z6qcuGy++zp0bb/yJYQy+XdcFu6LSf6QXQJA0YAAKqacBqL1RuugtAHzXBgWBoRjKAZS5w4NLd0NAOI2gACFi8Aau9a9buSUZUo9Yp9Jv0AlifJQJY5gL9NbwbGoYwggaAgNkasNrLEJ/Kx/QSqEitc2iYkgYMAAFTNmClF5iI05FYvwsDWOayei5EmWEKzeoZQQNAwJgGrPYyrV/TCzhFAY4xhWblegawzGUeS2XvH+X3p1/Rii9l+BnBbgdMO4ygASDgVANWexmtx3m9Cw+eNd82mUOzXhowAAQcNGCll2VUve+KmjgdidX6pUhdliV7GeZLKW7PwBoZQQNAwC/qL8tSgBnDPQpZnyXuBwxwORnMyhhBA0CABgw0w34s1kQDBoAADRhoS3XHgn+mF0CjBHBTelzgETagugyGEYygASBAA2YWzvZlZvZj0TwBzNRkL4u5dBQ99C/rn5d8MThgBA0AARowkxpXf/UKRlt0P9a7Uvx1ZTIaMAAEaMBM6i/l7wM/Q53gUl/Kotux1GAmIoAJ8o8YU1k4hMu74u8vlzKCBoAADZgU9YGJfSml3Cz49cyiuYwAJsC/Wcxlt2wGl/LOX2fGMoIGgAANmKXpC8xrV8qiNdjNkBhJAwaAAA2YhSnALGG39LFgGEwAs2J/pBdA0u5cBrtvCFlG0AAQoAEDq7UziKZiApj1MoFm+T3R0JsRNAAEaMDA6u2UYCqkAQNAgAYMbMFOCaY2AphK/PF7egWs3a4UKUxFjKABIEADZnqLXW3SeUYMtlOCqYUAZmIzpO9jzhpSM4ldEcLUwAgaAAI04MZ8Kh/TS7jQ74PmxobMzGOnBBOnAQNAgAZMO/bPVFKOuczu36Z5nl//Mc3zsDkCmJnMeF6v6AVWwAgaAAI0YCY2bJPVMKovU/pSJhpCwygaMMBFfv01vQLaJIABIMAIGtisL8UUmhwNGAACNGBgy5RgYgQwsHFfyqUh/GtxMQ6GM4IGgAANGMAkmgABDFCKDGZxRtAAEKABAzwYW4JtwWIMDRgAAjRggGdfSnEsmGUIYIAD9mOxBCNoAAjQgAFeU4KZnQCmYn+kF8CmXX6JSjjFCBoAAjRg6qUAE9dnEu0sYMbRgAEgQAOmFvoudbIdi5kIYIAzZDBzMIIGgAAB3JxP6QXAFn358iW9BNbGCBqgF+cFMy0NGAACNODG/JpeAGyaFsx0NGAACNCA29FK+f3dGb2s20ELdiEsRhLATWgle2ErnBnM5YygASBAA66f+gs1UoK5kACumuyFmslgLmEEDQABGnC1tF9owJdSbtJroFEaMAAEaMA1Un6hIbuH/94E10CLBHBlZC+0aleEMEMYQQNAgAZcD+UXmrdTgulNAFdhldn7R/m9+92wajsZTD9G0AAQoAHnrbL+HqP/sgU7JZgeNGAACNCAo5RfWKmdEsw5AjhmA+Erctm0XRHCnGIEDQABGnDCBsovUEopu5v0CqiXAF6Y7IVN2RlDc4wRNAAEaMDLUX5hk3ZFC6aLBgwAARrwIiYtv38vf5ny6YD57ZRgXhHAszN5BooM5hUjaAAIePMmvYJVm6n9Vj+CdgksOOImvYAnv6UXgBH0bIyegVd2NWUwWUbQABCgAc9B+QWO2hUtmFI0YACI0IAntkD5/XP+LwHMa6cEI4AntMjgWfrCSuxk8NYZQQNAgAY8iYV2Xam/sCo7JXjTBPDFlglf2QurtJPB22UEDQABGvAlljrfV/2FFdsVLXibNGAACNCAR3PsF5jKTgneIAE8htEzMLWdDN4aI2gACNCAB1ruPgvqL2zNTgneFAHcn+wF5raTwdthBA0AARpwL0ve4Ff9hW3bFS14GzRgAAjQgM/SfoGl7W7SK2B+AviUJbN3PeH7R3oBsAI7Y+j1M4IGgAAN+IhFy+962q/6C5PZFS143d68Sa+gRtWPnn+ffBWTkL4wvZuZnve3mZ6X3oygASBAAz7Qzui5thas/sJcbuZ4Ug04TgMGgAAN+Nmy5ffynVcVVWDtF2Z2M/kzasBxAriU0l72PqglguUvLOBm2qcTwHFG0AAQoAEv3n6nPem3ihKsAcNCbqZ7Kg04btsBvHj2znTJjXQKC2BYzs1EzyOA44ygASBgs5eiXEv5BTZm5wKVa6EBA0DANhuw+gs0bKcEr8LmNmGtM3vDu7BswoKAm8s+3SasOCNoAAjY0gg6UH6NnoG57IpJdNs2EsCR7N1E+po+Q9JOBjfMCBoAAjbQgDPtV/kFFrFTglulAQNAwLobcOjQ7/rrr/ILNdkpwU1a7XnAqezNp+/c5wQLX6jTzbCHOw84zggaAALW2IC3W367ja/Eyi405WbIgzXguNUFsPTt4zCTJS2sxE3vRwrgOCNoAAhYUwNWfgH6lmANOE4DBoCAlTTgXPlVf4H63Jx/iAYct4IAFr4Ar9yc+bgAjjOCBoCAthtwsvxqv0Dlbk59UAOOazaAo9krfIE23Bz9iACOM4IGgIAWG3C2/Kq/QFNuut+tAcdpwAAQ0FgDVn4Bhrt5/S4NOK6lAJa+AGPdHLwtgOOMoAEgoJEGrPwCXOrm5RsacFwDAZwOX+kLrMbN068EcJwRNAAE1N2A4+VX/QXW5ub+PxpwnAYMAAHVNmDlF2AuNxpwBX5JL6BDBdkrfYE12wngChhBA0BAZQ24ivKr/gIwu4oCuI7wlb0ALMEIGgACKmnAdbRf9ReApWjAABBQQQOuo/0qvwAsKRvAdWSv9AVgcUbQABAQa8C1lF/1F4CERADLXgA2zwgaAAIWbsD1lF/1F4AkDRgAAhZswNovADxaKICFLwC8ZAQNAAFv3sz+JbRfgPr8Lb2AzZt3BF1T9gpfACpiBA0AAbM14KrKr/YLQGU0YAAImKMB11V+1V8AKjRxAMteAOjDCBoAAiZswLW1X/UXgHpNE8DVZa/0BaBuRtAAEDDBpSirq7/KL0APLkaZpQEDQMBlx4CrK7/qLwBtGB3AshcAxjOCBoCAMZuwKiy/6i/AcLZhJQ0cQVeZvdIXgOYYQQNAwIARdJ3tV/kFGM0MOkgDBoCAfg24zvKr/gJcSgeOObsJq9bslb4AtMwIGgACTjfgWuuv8gtA444GcK3ZK30BWAMjaAAI6GrAyi8AzEwDBoCAgwZcb/lVfwFYkxcBLHwBYClG0AAQcN+Aay6/2i8AK/RL1dkrfAFYKSNoAAh48z69gqO0X4D5uR1SigYMAAFnb0eYof0CsG41BrD0BWD1jKABIKCyBqz8ArANNQWw9AVgM4ygASCgmgas/gKwJRowAATU0ICVXwA2Jx7A0heALTKCBoCA5M0YlF+ACrgdQ0ZsBC19AdgyI2gACMg0YPUXgI3TgAEgYPEGrPwCwNIBLH0BoJRiBA0AEQs2YPUXAB4tE8CyFwD2GEEDQMACDVj9BYBDGjAABMzbgJVfAOg0YwBLXwA4xggaAAJmasDaLwCcMkMAC18AOMcIGgACJm7A2i8A9KEBA0DAlA1Y/QWAniYKYNkLAEMYQQNAwBQNWP0FgIEuDGDZCwBjGEEDQMAlDVj9BYCRNGAACBjdgNVfABhvTADLXgC4kBE0AAS8eT/wE9RfgLX5W3oBmzRsBC19AWASRtAAENC7ASu/ADAdDRgAAvo1YPUXACZ1NoBlLwBMzwgaAAJOnwes/gJsgROBA06MoKUvAMzFCBoAArpH0MovwMaYQi9NAwaAgI4GrP4CbJMWvKSDABa+ABsmgRdkBA0AAS9OQ9J+AWApDyNo4QtAMYRekBE0AAT8ov0CwPI0YAAIeCOBAXjmIPBS5C8ABAhgAAgwggZgjyH0MuQvAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAn5JLwCAmrgMx1I0YAAIEMAAECCAASBAAANAgAAGgAABDAABAhiAZ85CWowABoAAAQwAAQIYgGef0wvYDgEMAAECGAACBDAABAhgAAgQwAAQIIABIEAAA0CAAAaAAAEMAAECGIBnbsawGAEMAAECGAACBDAAz9yMYTECGAACBDAABLyRwAC8ZCP0MuQvAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAS4EhYAB1wLawnyFwACBDAABAhgAAgQwAAQIIABIEAAA0CAAAaAAAEMAAECGIADn9ML2AQBDAABAhgAAgQwAAQIYAAIEMAAECCAASDA/YABeM0tgWcnfwEgQAADQIARNACdTKHnJYABOEIEz0n+AkCABgzAcUrwbOQvAAQIYACOc2fC2QhgAI4zgp6NAAaAAAEMAAECGAACBDAABPySXgAA1bIFa0YaMAAECGAACBDAABxhAj0nAQwAAQIYAAIEMAAECGAACHAeMACdbMGalwYMAAEaMACt+9xiX9eAAViBz5/TKxhKAANAgBE0AKvQ2hxaAAOwFk1lsBE0AARowAB0aKdJ7vvczMo1YAAIeCOBAXitlR75YP8kpCYWbwQNwNo0sRlLAQaAACNoADrU3yD3dV0Iq+7fg/wFYKXqvjqlAAaAAJuwAFirqjdjacAAEGATFgCv1Vscjzp+xLfO34z8BWDl6tyMJYABIMAmLADWrsrNWBowABvwubo5tAAGgAAjaAA2obY5tAYMAAECGICtqOpAsAtxAPBaVcPavvrFay2/NfkLAAE2YQGwKbVsxtKAAdiaKo4FC2AACDCCBmB7KphDa8AAECCAAdik9IFgI2gANio7h9aAASBAAwZgu4IlWAMGYNNSx4IFMAAEuBkDAB3SZ8mOcUGVDfx25S8ABAhgAAgcB7YLGgDK57L0HFoDBoAAAQwApZSl59BG0ABwb9HLcmjAABDgPGAAOrV3JvBUI+RlfufyFwACBDAAqzDdNZ2X2YxlExYA7FtkM5YGDAABNmEBcERb27CmHhzP/buXvwDQYe77BAtgAAiwCQsAOs27F0sDBoAAm7AAOKapXVhzHbKd60WQvwBwwlybsQQwAAQIYACOmftMnEbM8yrYBQ3ACsz6o8Is26E1YAAIEMAAcNb003gBDAABzgMG4IRGTgVeZrfYpC+G/AWAfiadQwtgAAgwggbgpCaG0AuesDzV6yGAATij+ghe+Hoh07we8hcAAlwJCwAGmebCWBowAAQIYADOqP2WDIH1TfAlbcIC4Lyq92GlfkC47EWRvwAQYBMWAE3LDcgv24ylAQNwXu2HgVMueF0EMAAE2IQFQC917sPKV/Oxr4v8BYAADRiAnmrswPkGXEa+MHZBA9CuKvJ33HZoBRgAAoygAeirthl0Hf33wdAXRwAD0F9VEVxV/pahL478BYAADRiAAWqqwLU14DLo5ZG/ABCgAQMwSC0duML+W0rp//oIYAAGqiGCa43fUvq+PvIXAAI0YACGylfgmgtwKb1eIQEMwHDZCK49fkufF0j+AkCABgzACMkK3EABLudfIfkLAAEaMACjpDpwG/23lHLmJRLAAIwVyOCG4reUk6+Q/AWAAA0YgPGW7sCNFeBSjr9EAhiACyyZwA2mbynl2GskfwEgQAMG4EJLtOBW2++DjpdI/gJAgAAG4EILtNPGC3DX+o2gAZjAnGPo1tP33uErJH8BIEADBmAi87TgdfTfUg5fHwEMwFSmTuD1ZO+9vddH/gJAgAYMwISmK8Frq7/3nl8f+QsAARowAFO7vAavs/7uEcAAzGRcDm8ge0spRtAAEKEBAzCrAT14K+W3lCKAAVjI6SDeVPaWUoygASBCAwZgaXtleHvd9578BYAADRgAAuQvAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAgAAGgAABDAABAhgAAgQwAAQIYAAIEMAAECCAASBAAANAwP8Cu63jH+gFQUUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=1920x1080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "im = train_ds[0][\"label\"]\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = f\"datasets/{hf_dataset_identifier}\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/dawon/TIL2023/tilvenv/lib/python3.7/site-packages/transformers/models/segformer/feature_extraction_segformer.py:31: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import SegformerFeatureExtractor\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor()\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1) \n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.num_batches_tracked']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.00006\n",
    "batch_size = 2\n",
    "\n",
    "hub_model_id = \"segformer-b0-finetuned-segments-sidewalk-2\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"segformer-b0-finetuned-segments-sidewalk-outputs\",\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"end\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f081409062d34155b2cff7aa50db2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    # currently using _compute instead of compute\n",
    "    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "    metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=0,\n",
    "            reduce_labels=feature_extractor.do_reduce_labels,\n",
    "        )\n",
    "    \n",
    "    # add per category metrics as individual key-value pairs\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/leadawon/segformer-b0-finetuned-segments-sidewalk-2 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tilvenv",
   "language": "python",
   "name": "tilvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
